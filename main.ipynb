{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled12.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMXcK0TYeFiQI4M8GwgzmJx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zcQ1Gi9NLw1B"},"outputs":[],"source":["import numpy as np\n","import math\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch\n","import os\n","\n","def weights_init_normal(m):\n","  classname = m.__class__.__name__\n","  if classname.find('Conv') != -1:\n","    torch.nn.init.normal_(m.weight.data,0.0,0.02)\n","  elif classname.find('BatchNorm2d') != -1:\n","    torch.nn.init.normal_(m.weight.data,1.0,0.02)\n","    torch.nn.init.constant_(m.bias.data,0.0)\n","\n","class Generator(nn.Module):\n","  def __init__(self):\n","    super(Generator,self).__init__()\n","    self.init_size = 8\n","    self.l = nn.Sequential(nn.Linear(100,128*(self.init_size**2)))\n","    self.conv = nn.Sequential(\n","        nn.BatchNorm2d(128),\n","        nn.Upsample(scale_factor = 2),\n","        nn.Conv2d(128,128,3,stride = 1, padding = 1),\n","        nn.BatchNorm2d(128,0.8),\n","        nn.LeakyReLU(0.2 , inplace = True),\n","        nn.Upsample(scale_factor = 2),\n","        nn.Conv2d(128,64,3,stride = 1, padding = 1),\n","        nn.BatchNorm2d(64,0.8),\n","        nn.LeakyReLU(0.2,inplace = True),\n","        nn.Conv2d(64,3,3,stride = 1, padding = 1),\n","        nn.Tanh())\n","\n","       \n","  def forward(self,r):\n","    r2 = self.l(r)\n","    r3 = r2.view(r2.shape[0],128,self.init_size,self.init_size)\n","    output = self.conv(r3)\n","    return output\n","\n","class Discriminator(nn.Module):\n","  def __init__(self):\n","    super(Discriminator,self).__init__()\n","\n","    def discriminator_conv(in_feature_num, out_feature_num,bn = True):\n","      conv = [nn.Conv2d(in_feature_num, out_feature_num,3,2,1), nn.LeakyReLU(0.2, inplace = True), nn.Dropout2d(0.25)]\n","      if bn:\n","        conv.append(nn.BatchNorm2d(out_feature_num,0.8))\n","      return conv\n","    self.network = nn.Sequential(\n","        *discriminator_conv(3,16,bn = False),\n","        *discriminator_conv(16,32),\n","        *discriminator_conv(32,64),\n","        *discriminator_conv(64,128)\n","    )\n","    self.validation_layer = nn.Sequential(nn.Linear(128*(32**2),1), nn.Sigmoid())\n","\n","    def forward(self,img):\n","      img1 = self.network(img)\n","      img2 = img1.view(img1.shape[0],-1)\n","      validity = self.validation_layer(img2)\n","\n","      return validity\n","    \n","\n","adversarial_loss = torch.nn.BCELoss()\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","generator.apply(weights_init_normal)\n","discriminator.apply(weights_init_normal)\n","\n","\n","data = torch.utils.data.DataLoader(\n","    datasets.MNIST(\n","        \n","        train = True,\n","       transform = transforms.Compose(\n","           [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize([0.5],[0.5])]\n","       )),\n","        batch_size = 64,\n","        shuffle = True\n","    )\n","\n","g_optimizer = torch.optim.Adam(generator.parameters(),lr = 0.0002, betas = (0.5,0.999))\n","d_optimizer = torch.optim(discriminator.parameters(), lr = 0.0002, betas = (0.5,0.999))\n","Tensor = torch.FloatTensor\n","epochs = 3\n","\n","for epoch in range(epochs):\n","  for i , (imgs,_) in enumerate(data):\n","    valid = Variable(Tensor(imgs.shape[0],1).fill_(1.0), requires_grad = False)\n","    fake = Variable(Tensor(imgs.shape[0],1).fill_(0.0), requires_grad = False)\n","    original_imgs = Variable(imgs.type(Tensor))\n","g_optimizer.zero_grad()\n","\n","noise = Variable(Tensor(np.random.normal(0,1,(imgs.shape[0],100))))\n","\n","gen_imgs = generator(noise)\n","g_loss = adversarial_loss(discriminator(gen_imgs),valid)\n","\n","g_loss.backward()\n","g_optimizer.step()\n","\n","d_optimizer.zero_grad()\n","\n","real_loss = adversarial_loss(discriminator(original_imgs), valid)\n","fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n","d_loss = (real_loss + fake_loss)/2\n","\n","d_loss.backward()\n","d_optimizer.step()\n","\n","\n","\n","    \n","\n","    \n"]}]}